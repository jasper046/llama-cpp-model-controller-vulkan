# NOTE: WORK IN PROGRESS, VULKAN CONVERSION IS ONGOING

# Llama.cpp Model Controller ü¶ô

This is a fork of [Dan-Duran's Llama.cpp Model Controller](https://github.com/Dan-Duran/llama-cpp-model-controller), that supports cuda. I will convert it to use vulkan (for multi/mixed-new/old gpu support)


The Llama.cpp Model Controller is an intuitive web interface for managing local LLM deployments powered by llama.cpp. This application streamlines the process of starting, monitoring, and stopping language models through a clean, responsive UI, eliminating the need for complex command-line operations.

Key features include real-time GPU monitoring with temperature and memory usage statistics, color-coded live server logs showing token usage and model output, and customizable deployment parameters. Users can easily configure thread count, context size, and GPU allocation strategies for optimal performance on their hardware.

## üìã Requirements

- Linux environment with CUDA support
- NVIDIA GPU(s) with appropriate drivers
- Python 3.8+ 
- llama.cpp compiled with CUDA support
- GGUF format models

## üöÄ Installation

### 1. Clone the repository

```bash
git clone https://github.com/Dan-Duran/llama-cpp-model-controller.git
cd llama-cpp-model-controller
```

### 2. Create a virtual environment

```bash
python -m venv venv
source venv/bin/activate
```

### 3. Install dependencies

```bash
pip install flask
```

### 4. Set up model directory

By default, the application looks for models in a `models` directory. Make sure your GGUF models are placed there:

```bash
mkdir -p models
# Copy or download your GGUF models to the models directory
```

### 5. Configure the application

Edit `app.py` to customize the path variables at the top of the file:

```python
# Configuration variables - update these to match your environment
HOME_DIR = os.path.expanduser("~")  # Dynamically get user's home directory
PROJECT_DIR = os.path.join(HOME_DIR, "llama")  # Project directory
MODEL_DIR = os.path.join(PROJECT_DIR, "models")  # Model directory
LLAMA_CPP_PATH = os.path.join(PROJECT_DIR, "llama.cpp-b4823/build/bin/llama-server")  # Path to llama-server executable
CACHE_DIR = os.path.join(HOME_DIR, ".cache/llama")  # Llama cache directory
```

You can modify these paths to match your specific environment:
- `HOME_DIR`: Your home directory (automatically detected)
- `PROJECT_DIR`: The directory where the application is installed
- `MODEL_DIR`: Where your GGUF models are stored
- `LLAMA_CPP_PATH`: Path to the llama-server executable
- `CACHE_DIR`: Where llama.cpp stores its cache files


## üñ•Ô∏è Usage

### Starting the Web UI

```bash
python app.py
```

The web interface will be available at `http://localhost:5000` by default.

### Deploying a Model

1. Select a model from the dropdown menu
2. Configure the parameters (or use the defaults)
   - **Threads**: Number of CPU threads to use (recommended: 16)
   - **GPU Layers**: Number of layers to offload to GPU (default: 99)
   - **Context Size**: Token context window size (default: 32000)
   - **Split Mode**: How to divide the model across GPUs (layer or row)
   - **Parallel Sequences**: Number of sequences to process in parallel
3. Click "Start Model"
4. The model will be available at the configured host:port

### Monitoring

- The GPU stats section shows current GPU utilization
- The Server Logs section displays real-time output from the llama-server process
- Color-coded logs help identify errors, warnings, and token usage information

### Stopping a Model

Click the "Stop Model" button to terminate the server and clear the cache.

## üìÅ Project Structure

```
llama-cpp-web-ui/
‚îú‚îÄ‚îÄ app.py                  # Flask application
‚îú‚îÄ‚îÄ templates/              # HTML templates
‚îÇ   ‚îî‚îÄ‚îÄ index.html          # Main UI template
‚îú‚îÄ‚îÄ models/                 # GGUF model files (not included in repo)
‚îî‚îÄ‚îÄ venv/                   # Python virtual environment
```

## ‚öôÔ∏è Advanced Configuration

### Custom Port

To change the port the web UI runs on, modify the last line in `app.py`:

```python
app.run(host="0.0.0.0", port=5000, debug=True)
```

### Adding Model Presets

You can create preset configurations for each model by modifying the HTML template.

## üîß Troubleshooting

### Common Issues

1. **Model fails to load**
   - Ensure you have enough GPU memory
   - Try reducing context size or using fewer GPU layers
   - Check model file permissions
   - Verify the `MODEL_DIR` path is correct and models exist at that location

2. **GPU not detected**
   - Verify CUDA installation
   - Check nvidia-smi output
   - Ensure llama.cpp was compiled with CUDA support

3. **Out of memory errors**
   - Reduce context size
   - Use a smaller model
   - Adjust split mode settings

4. **Llama-server executable not found**
   - Check that the `LLAMA_CPP_PATH` points to your compiled llama-server executable
   - Make sure llama.cpp is compiled with CUDA support
   - Verify execution permissions with `chmod +x [path-to-llama-server]`

5. **Path configuration issues**
   - The application logs the paths it's using on startup - check these logs
   - Ensure all directories exist and are accessible
   - If running in a container or different environment, update paths accordingly

## ü§ù Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## üìÑ License

This project is licensed under the [MIT License](LICENSE) - see the LICENSE file for details.

## üôè Acknowledgments

- [llama.cpp](https://github.com/ggerganov/llama.cpp) for the incredible optimized LLM implementation
- All the model creators and fine-tuners who make their work available
